# Conclusion

In this project, our main goal was to achieve Automatic transcription for multiple instruments using deep learning approaches. We further divided this task into three sub-tasks. The first was to separate out each source from a musical mixture. We focused on an approach to separate two musical sources, specifically vocals and background instruments. We presented an LSTM based model, with a specially designed discriminative loss function to achieve this objective. Next we presented a CONVNET approach to identify the predominant instrument in a musical piece. The purpose of this part was to aid in deciding which transcription model should be used for a musical piece based on the corresponding predominant instrument. Finally, we presented a CONVET based approach for transcribing piano music. For each section, we show that our models achieve promising results. In the next section, we discuss some of the future directions of this project, to transform it into the product orginally envisioned.

## Future Work

There are several directions of future work. One, would be to improve upon the source separation model to extend it to identify other sources as well. This would entail a different model architecture and a different loss function. Another direction would be make a flexible transcription model, that can transcribe for any musical source. Both, these extensions are highly complex and hence are beyond the scope of our current project objective.
